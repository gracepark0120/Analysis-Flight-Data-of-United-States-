{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "key fields we need to look at\n",
    "@acid (Flight Number)\n",
    "@airline (Airline)\n",
    "@arrArpt (Arrival Airport)\n",
    "@depArpt (Departure Airport)\n",
    "fdm:trackInformation.nxcm:ncsmTrackData.nxcm:departureFixAndTime.@arrTime (Departure Time)\n",
    "fdm:trackInformation.nxcm:ncsmTrackData.nxcm:eta.@timeValue (Scheduled Arrival Time)\n",
    "fdm:trackInformation.nxcm:ncsmTrackData.nxcm:arrivalFixAndTime.@arrTime (Actual Arrival Time)\n",
    "\n",
    "TODO LIST\n",
    "[o] do these operations on all the json files, not just flights_000\n",
    "[o] convert the timestamp (currently stored as a string) to some time value to do arithmetic, and determine the delay for each recorded flight\n",
    "[] create another table with airport hubs for each airline to compare data?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/11/27 02:12:15 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------+---------------+-----------------+--------------------+----------------------+--------------------+-------+----------+--------+--------+\n",
      "|Flight Number|Airline|Arrival Airport|Departure Airport|      Departure Time|Scheduled Arrival Time| Actual Arrival Time|Delayed|Passengers|Distance|ItinFare|\n",
      "+-------------+-------+---------------+-----------------+--------------------+----------------------+--------------------+-------+----------+--------+--------+\n",
      "|      UAL1722|    UAL|            PHL|              SFO|2022-10-22T06:52:26Z|  2022-10-22T11:29:42Z|2022-10-22T11:13:58Z|  false|       1.0|  2521.0|   221.0|\n",
      "|      DAL1198|    DAL|            ATL|              SAN|2022-10-22T06:04:55Z|  2022-10-22T09:17:59Z|2022-10-22T09:00:43Z|  false|       1.0|  1892.0|   774.0|\n",
      "|      UAL1126|    UAL|            ORD|              PHX|2022-10-22T07:16:00Z|  2022-10-22T09:50:47Z|2022-10-22T09:32:48Z|  false|       3.0|  1440.0|   438.0|\n",
      "|      UAL1200|    UAL|            IAD|              LAX|2022-10-22T05:00:00Z|  2022-10-22T08:56:00Z|2022-10-22T08:42:27Z|  false|       1.0|  2288.0|   708.0|\n",
      "|      UAL2059|    UAL|            EWR|              SFO|2022-10-22T06:18:42Z|  2022-10-22T10:47:58Z|2022-10-22T10:36:24Z|  false|       4.0|  2565.0|   150.0|\n",
      "|      FFT1418|    FFT|            ORD|              LAS|2022-10-22T07:14:00Z|  2022-10-22T10:03:31Z|2022-10-22T09:45:10Z|  false|       2.0|  1514.0|   139.0|\n",
      "|      AAL1542|    AAL|            DCA|              PHX|2022-10-22T07:20:47Z|  2022-10-22T10:46:03Z|2022-10-22T10:28:24Z|  false|       1.0|  1979.0|   782.0|\n",
      "|      UAL2059|    UAL|            EWR|              SFO|2022-10-22T06:18:42Z|  2022-10-22T10:47:59Z|2022-10-22T10:36:25Z|  false|       2.0|  2565.0|   425.0|\n",
      "|       ASA790|    ASA|            EWR|              SEA|2022-10-22T05:33:00Z|  2022-10-22T09:44:25Z|2022-10-22T09:33:07Z|  false|       7.0|  2402.0|   144.0|\n",
      "|       ASA339|    ASA|            JFK|              SFO|2022-10-22T07:05:00Z|  2022-10-22T11:43:00Z|2022-10-22T11:35:52Z|  false|      31.0|  2586.0|   144.0|\n",
      "+-------------+-------+---------------+-----------------+--------------------+----------------------+--------------------+-------+----------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timezone\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import UserDefinedFunction, to_timestamp\n",
    "from pyspark.sql.types import StringType,IntegerType\n",
    "from pyspark.sql.functions import collect_list,split,regexp_replace,col,round,concat,lit,avg,when,length\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('flight-data').getOrCreate()\n",
    "flight_data = spark.read.format(\"json\").options(inferschema='true',header='true').load('../part1/flights/flights_000.jsonl')\n",
    "flight_data = flight_data.select('@acid', '@airline', '@arrArpt', '@depArpt',\\\n",
    "                                 'fdm:trackInformation.nxcm:ncsmTrackData.nxcm:departureFixAndTime.@arrTime',\\\n",
    "                                 'fdm:trackInformation.nxcm:ncsmTrackData.nxcm:eta.@timeValue',\\\n",
    "                                 'fdm:trackInformation.nxcm:ncsmTrackData.nxcm:arrivalFixAndTime.@arrTime')\n",
    "# renames the table columns\n",
    "new_column_names = [\"Flight Number\", \"Airline\", \"Arrival Airport\", \"Departure Airport\", \"Departure Time\", \"Scheduled Arrival Time\", \"Actual Arrival Time\"]\n",
    "flight_data = flight_data.toDF(*new_column_names)\n",
    "# removing invalid departure and arrival times, null values\n",
    "flight_data = flight_data.na.drop()\n",
    "# removes placeholder letter in front of airport code, if it exists\n",
    "flight_data = flight_data.withColumn('Arrival Airport', when(length(col('Arrival Airport')) == 4, col('Arrival Airport').substr(2,3))\\\n",
    "                         .otherwise(col('Arrival Airport')))\\\n",
    "                         .withColumn('Departure Airport', when(length(col('Departure Airport')) == 4, col('Departure Airport').substr(2,3))\\\n",
    "                         .otherwise(col('Departure Airport')))\\\n",
    "                         .withColumn('Delayed', to_timestamp(flight_data[\"Scheduled Arrival Time\"]).cast('long') < to_timestamp(flight_data[\"Actual Arrival Time\"]).cast('long'))\n",
    "                         \n",
    "# flight_data.show(10)\n",
    "\n",
    "fares = spark.read.parquet('./ticket_fares/output.parquet/')\n",
    "flight_data = flight_data.join(fares, (flight_data.Airline == fares.Airline) \n",
    "                            & (flight_data['Arrival Airport'] == fares.Origin)\n",
    "                            & (flight_data['Departure Airport'] == fares.Dest))\\\n",
    "                    .drop(fares.Airline).drop('ItinID', 'Origin', 'Dest')\\\n",
    "                    .dropDuplicates()\n",
    "# flight_data.show(10)\n",
    "# flight_data.write.csv(\"./temp.csv\", mode='overwrite')\n",
    "\n",
    "flight_data.write\\\n",
    "        .format('jdbc')\\\n",
    "        .option('url', 'jdbc:postgresql://localhost:5432/cs179g')\\\n",
    "        .option('dbtable', \"flight_data\")\\\n",
    "        .option(\"user\", \"group9\") \\\n",
    "        .option(\"password\", \"group9\") \\\n",
    "        .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "        .mode('overwrite')\\\n",
    "        .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import Row\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"Test\").getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "def has_column(df, col):\n",
    "    try:\n",
    "        df[col]\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "flight_data = spark.read.format(\"json\").options(inferschema='true',header='true').load('../part1/flights/flights_000.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+--------+--------+--------+----------+----------+--------+------------+---------------+----------------+----------------------+--------------------------+------------------------+----------------------------------+--------------------------+-------------------------+--------------------+--------------------+-------------------+------------------------------+---------------------+-------------------+-----------------+--------------------+------------+\n",
      "|@acid|@airline|@arrArpt|@cdmPart|@depArpt|@fdTrigger|@flightRef|@msgType|@sensitivity|@sourceFacility|@sourceTimeStamp|fdm:arrivalInformation|fdm:boundaryCrossingUpdate|fdm:departureInformation|fdm:flightPlanAmendmentInformation|fdm:flightPlanCancellation|fdm:flightPlanInformation|fdm:ncsmFlightCreate|fdm:ncsmFlightModify|fdm:ncsmFlightRoute|fdm:ncsmFlightScheduleActivate|fdm:ncsmFlightSectors|fdm:ncsmFlightTimes|fdm:oceanicReport|fdm:trackInformation|ResponseType|\n",
      "+-----+--------+--------+--------+--------+----------+----------+--------+------------+---------------+----------------+----------------------+--------------------------+------------------------+----------------------------------+--------------------------+-------------------------+--------------------+--------------------+-------------------+------------------------------+---------------------+-------------------+-----------------+--------------------+------------+\n",
      "+-----+--------+--------+--------+--------+----------+----------+--------+------------+---------------+----------------+----------------------+--------------------------+------------------------+----------------------------------+--------------------------+-------------------------+--------------------+--------------------+-------------------+------------------------------+---------------------+-------------------+-----------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col,lit, when\n",
    "from pyspark.sql.types import BooleanType\n",
    "from pyspark.sql.utils import AnalysisException\n",
    "from pyspark.sql import Row\n",
    "\n",
    "\n",
    "def has_column(df, col):\n",
    "    try:\n",
    "        df[col]\n",
    "        return True\n",
    "    except AnalysisException:\n",
    "        return False\n",
    "\n",
    "# has_column(flight_data, \"fdm:trackInformation.nxcm:airlineData\")\n",
    "\n",
    "if has_column(flight_data, \"*.nxcm:qualifiedAircraftId.nxce:igtd\"):\n",
    "# if has_column(flight_data, \"fdm:trackInformation.nxcm:airlineData.nxcm:flightTimeData.@originalArrival\"):\n",
    "    df_basicInfo = flight_data.withColumn(\"ResponseType\", col(\"*.nxcm:qualifiedAircraftId.nxce:igtd\"))\n",
    "    # df_basicInfo = flight_data.withColumn(\"ResponseType\", col(\"fdm:trackInformation.nxcm:airlineData.nxcm:flightTimeData.@originalArrival\"))\n",
    "else:\n",
    "    # Adjust types according to your needs\n",
    "    df_basicInfo = flight_data.withColumn(\"ResponseType\", lit(None).cast(\"string\"))\n",
    "df_basicInfo.filter(col(\"ResponseType\").isNotNull()).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "+-------------+-------+---------------+-----------------+-------------------+----------------------+-------------------+-------+----------+--------+--------+\n",
      "|Flight Number|Airline|Arrival Airport|Departure Airport|     Departure Time|Estimated Arrival Time|Actual Arrival Time|Delayed|Passengers|Distance|ItinFare|\n",
      "+-------------+-------+---------------+-----------------+-------------------+----------------------+-------------------+-------+----------+--------+--------+\n",
      "|      UAL1722|    UAL|            PHL|              SFO|2022-10-22 06:52:26|   2022-10-22 11:29:42|2022-10-22 11:13:58|  false|       1.0|  2521.0|   221.0|\n",
      "|      DAL1198|    DAL|            ATL|              SAN|2022-10-22 06:04:55|   2022-10-22 09:17:59|2022-10-22 09:00:43|  false|       1.0|  1892.0|   774.0|\n",
      "|      UAL1126|    UAL|            ORD|              PHX|2022-10-22 07:16:00|   2022-10-22 09:50:47|2022-10-22 09:32:48|  false|       3.0|  1440.0|   438.0|\n",
      "|      UAL1200|    UAL|            IAD|              LAX|2022-10-22 05:00:00|   2022-10-22 08:56:00|2022-10-22 08:42:27|  false|       1.0|  2288.0|   708.0|\n",
      "|      UAL2059|    UAL|            EWR|              SFO|2022-10-22 06:18:42|   2022-10-22 10:47:58|2022-10-22 10:36:24|  false|       4.0|  2565.0|   150.0|\n",
      "+-------------+-------+---------------+-----------------+-------------------+----------------------+-------------------+-------+----------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "spark = SparkSession.builder.master(\"local[1]\").appName(\"PSQL Write Test\")\\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/home/ubuntu/postgresql-42.5.0.jar\")\\\n",
    "    .getOrCreate()\n",
    "\n",
    "data_path = \"./temp.csv\"\n",
    "data = spark.read.csv(data_path, header=False, inferSchema=True).limit(5)\n",
    "new_column_names = [\"Flight Number\", \"Airline\", \"Arrival Airport\", \"Departure Airport\", \"Departure Time\", \"Estimated Arrival Time\", \"Actual Arrival Time\", \"Delayed\", \"Passengers\", \"Distance\", \"ItinFare\"]\n",
    "data = data.toDF(*new_column_names)\n",
    "print(data.count())\n",
    "data.show(10)\n",
    "\n",
    "data.write\\\n",
    "\t.format('jdbc')\\\n",
    "\t.option('url', 'jdbc:postgresql://localhost:5432/cs179g')\\\n",
    "\t.option('dbtable', \"students\")\\\n",
    "\t.option(\"user\", \"group9\") \\\n",
    "\t.option(\"password\", \"group9\") \\\n",
    "\t.option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "\t.mode('overwrite')\\\n",
    "\t.save()\n",
    "\n",
    "# df = spark.read \\\n",
    "#     .format(\"jdbc\") \\\n",
    "#     .option(\"url\", url) \\\n",
    "#     .option(\"dbtable\", \"students\") \\\n",
    "#     .option(\"user\", \"group9\") \\\n",
    "#     .option(\"password\", \"group9\") \\\n",
    "#     .option(\"driver\", \"org.postgresql.Driver\") \\\n",
    "#     .load()\n",
    "# df.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('cs179g_venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "29cfaa6afbbb70b67ddedb8599d785bf882cf780961a15813e9a3d9fdce6c5b0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
